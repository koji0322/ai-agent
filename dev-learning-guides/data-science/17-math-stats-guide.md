# DS のための数学・統計ガイド

## 0. はじめに

このガイドは **データサイエンスに必要な数学・統計の要点と学習ロードマップをまとめた自分用リファレンス** です。

- **想定読者**: DS・ML を学んでいるが、数学・統計の基礎に不安がある人
- **ゴール**: ML の背後にある数学・統計の要点を理解し、適切な学習リソースを選べるようになる
- **前提**: 高校数学（二次関数・確率の基礎）レベルを理解していること
- **関連ガイド**: [12 - DS 向け Web サイトガイド](12-web-resources-guide.md)、[15 - DS ワークフローガイド](15-workflow-guide.md)

> **このガイドの位置づけ**: 数学・統計の教科書ではなく、**DS に必要な範囲を俯瞰し、学習の優先度と推奨リソースを示すガイド**。
> 各トピックの詳細は推奨リソースで学ぶ。

### セクション一覧

| セクション | 内容 |
|-----------|------|
| [1. 学習ロードマップ](#1-学習ロードマップ) | 何をどの順で学ぶか |
| [2. 線形代数](#2-線形代数) | ベクトル・行列・固有値 |
| [3. 微積分・最適化](#3-微積分最適化) | 微分・勾配降下法・損失関数 |
| [4. 確率](#4-確率) | 確率分布・ベイズの定理・期待値 |
| [5. 統計学の基礎](#5-統計学の基礎) | 記述統計・推測統計・検定 |
| [6. 統計的検定](#6-統計的検定) | t 検定・カイ二乗検定・A/B テスト |
| [7. 回帰分析](#7-回帰分析) | 線形回帰・ロジスティック回帰・正則化 |
| [8. 推奨学習リソース](#8-推奨学習リソース) | 書籍・動画・Web サイト |
| [9. まとめ — 必要な数学レベル](#9-まとめ--必要な数学レベル) | タスク別の必要度マップ |

---

## 1. 学習ロードマップ

すべてを一度に学ぶ必要はない。DS のタスクに応じて段階的に学ぶ。

### 優先度付き学習順序

| 優先度 | 分野 | DS での用途 | 学習時間の目安 |
|--------|------|-----------|-------------|
| 1（必須） | 記述統計 | EDA・データの要約 | 1〜2 週間 |
| 1（必須） | 確率の基礎 | 確率分布・モデルの理解 | 1〜2 週間 |
| 2（重要） | 線形代数の基礎 | ML アルゴリズムの理解 | 2〜3 週間 |
| 2（重要） | 推測統計 | 仮説検定・信頼区間 | 2〜3 週間 |
| 3（応用） | 微積分・最適化 | 勾配降下法・DL の理解 | 2〜3 週間 |
| 3（応用） | 回帰分析 | 線形回帰・ロジスティック回帰 | 1〜2 週間 |
| 4（発展） | ベイズ統計 | ベイズモデリング・確率的プログラミング | 3〜4 週間 |
| 4（発展） | 情報理論 | 交差エントロピー・KL ダイバージェンス | 1〜2 週間 |

### タスク別に必要な数学

| タスク | 必要な数学 |
|--------|-----------|
| EDA・データ分析 | 記述統計・確率の基礎 |
| ML（scikit-learn） | 線形代数の基礎・確率・統計 |
| ディープラーニング | 線形代数・微積分・最適化 |
| A/B テスト | 推測統計・仮説検定 |
| 時系列分析 | 統計学・確率過程 |
| NLP | 線形代数・確率・情報理論 |
| 推薦システム | 線形代数（行列分解） |

---

## 2. 線形代数

ML のほぼすべてのアルゴリズムは線形代数の上に成り立っている。

### DS に必要な線形代数の要点

| 概念 | DS での用途 | 直感的な理解 |
|------|-----------|-------------|
| **ベクトル** | 特徴量ベクトル・埋め込み | データ 1 件を数値の列として表現 |
| **行列** | データセット全体・重み行列 | 特徴量 x サンプル数のテーブル |
| **行列の積** | ニューラルネットワークの計算 | 入力を重みで変換する操作 |
| **転置** | データの形状変換 | 行と列を入れ替える |
| **逆行列** | 線形回帰の解析解 | 連立方程式を解く道具 |
| **固有値・固有ベクトル** | PCA（主成分分析） | データの分散が最大になる方向 |
| **特異値分解（SVD）** | 次元削減・推薦システム | 行列を 3 つの行列の積に分解 |
| **内積** | 類似度の計算 | 2 つのベクトルがどれだけ似ているか |
| **ノルム** | 正則化（L1・L2） | ベクトルの大きさ（長さ） |

### Python での線形代数

```python
import numpy as np

# ベクトル
v = np.array([1, 2, 3])
w = np.array([4, 5, 6])

# 内積
dot = np.dot(v, w)           # 32

# ノルム
l2_norm = np.linalg.norm(v)  # ユークリッドノルム
l1_norm = np.linalg.norm(v, ord=1)  # L1 ノルム

# 行列
A = np.array([[1, 2], [3, 4]])

# 行列の積
B = np.array([[5, 6], [7, 8]])
C = A @ B                    # または np.dot(A, B)

# 転置
A_T = A.T

# 逆行列
A_inv = np.linalg.inv(A)

# 固有値・固有ベクトル
eigenvalues, eigenvectors = np.linalg.eig(A)

# 特異値分解（SVD）
U, S, Vt = np.linalg.svd(A)
```

### PCA と線形代数の関係

```python
from sklearn.decomposition import PCA

# PCA の実行
pca = PCA(n_components=2)
X_reduced = pca.fit_transform(X)

# 内部では以下が行われている:
# 1. データの標準化
# 2. 共分散行列の計算
# 3. 固有値分解で主成分（分散が最大の方向）を抽出
# 4. 上位 k 個の固有ベクトルに射影
```

---

## 3. 微積分・最適化

ディープラーニングの学習アルゴリズム（勾配降下法）は微積分がベース。

### DS に必要な微積分の要点

| 概念 | DS での用途 | 直感的な理解 |
|------|-----------|-------------|
| **微分** | 勾配の計算 | 関数の傾き。「どっちに動けば値が減るか」 |
| **偏微分** | 多変数関数の勾配 | 各パラメータに対する傾き |
| **勾配（gradient）** | 勾配降下法 | 全パラメータの偏微分をベクトルにまとめたもの |
| **連鎖律（chain rule）** | バックプロパゲーション | 合成関数の微分法則。DL の学習の核心 |

### 勾配降下法の直感

```
目的: 損失関数 L(w) を最小にするパラメータ w を見つける

1. パラメータ w をランダムに初期化
2. 勾配（傾き）を計算: dL/dw
3. 勾配の逆方向にパラメータを更新: w = w - learning_rate * dL/dw
4. 2〜3 を収束するまで繰り返す
```

**学習率の影響:**

| 学習率 | 挙動 |
|--------|------|
| 小さすぎる | 収束が遅い。局所最適に捕まりやすい |
| 適切 | 安定して最小値に近づく |
| 大きすぎる | 発散する（損失が増大する） |

### 損失関数

| 損失関数 | 用途 | 数式の直感 |
|---------|------|-----------|
| MSE（平均二乗誤差） | 回帰 | 予測値と正解の差の二乗の平均 |
| Cross-Entropy | 分類 | 予測確率と正解ラベルのズレ |
| MAE（平均絶対誤差） | 回帰（外れ値に頑健） | 予測値と正解の差の絶対値の平均 |

---

## 4. 確率

ML のモデルは確率的な予測を行う。確率の基礎はすべての ML に通じる。

### DS に必要な確率の要点

| 概念 | DS での用途 | 直感的な理解 |
|------|-----------|-------------|
| **確率分布** | データの生成過程のモデル化 | データがどのような形で散らばるか |
| **期待値・分散** | データの要約 | 平均的な値・ばらつきの大きさ |
| **条件付き確率** | ベイズの定理・分類 | ある条件のもとでの確率 |
| **ベイズの定理** | ナイーブベイズ・ベイズ推定 | 事前知識をデータで更新する |
| **尤度** | 最尤推定 | パラメータが与えられたとき、データが観測される確率 |

### 主要な確率分布

| 分布 | 種類 | DS での用途 |
|------|------|-----------|
| **正規分布** | 連続 | 自然現象のモデル化。中心極限定理。検定 |
| **ベルヌーイ分布** | 離散 | 二値分類の結果（成功/失敗） |
| **二項分布** | 離散 | n 回の試行での成功回数 |
| **ポアソン分布** | 離散 | 単位時間あたりのイベント発生回数 |
| **一様分布** | 連続 | ランダムサンプリング |
| **指数分布** | 連続 | イベント間の待ち時間 |

### ベイズの定理

```
P(A|B) = P(B|A) * P(A) / P(B)

DS での解釈:
P(仮説|データ) = P(データ|仮説) * P(仮説) / P(データ)

事後確率 = 尤度 x 事前確率 / 正規化定数
```

**実用例 — スパムフィルタ:**

```
P(スパム|"無料") = P("無料"|スパム) * P(スパム) / P("無料")

「"無料" という単語を含むメールがスパムである確率」
= 「スパムメールに "無料" が含まれる確率」x「スパムの事前確率」/ 「"無料" が出現する確率」
```

---

## 5. 統計学の基礎

### 記述統計 — データの要約

| 指標 | 説明 | Python |
|------|------|--------|
| **平均** | データの重心 | `df["col"].mean()` |
| **中央値** | 真ん中の値。外れ値に頑健 | `df["col"].median()` |
| **最頻値** | 最も出現頻度が高い値 | `df["col"].mode()` |
| **分散** | ばらつきの大きさ（平均からの偏差の二乗平均） | `df["col"].var()` |
| **標準偏差** | 分散の平方根。元のデータと同じ単位 | `df["col"].std()` |
| **四分位数** | データを 4 等分する値 | `df["col"].quantile([0.25, 0.5, 0.75])` |
| **歪度** | 分布の左右の非対称性 | `df["col"].skew()` |
| **尖度** | 分布の裾の重さ | `df["col"].kurtosis()` |

### 推測統計 — サンプルから母集団を推測

| 概念 | 説明 |
|------|------|
| **母集団** | 知りたい対象全体 |
| **標本（サンプル）** | 母集団から抽出した一部のデータ |
| **点推定** | 母数をサンプルから 1 つの値で推定（例: 標本平均 → 母平均） |
| **区間推定** | 母数が含まれる範囲を確率付きで推定（信頼区間） |
| **信頼区間** | 「95% 信頼区間」= 同じ方法で 100 回推定したら、95 回はこの区間に母数が含まれる |

### 信頼区間の計算

```python
import numpy as np
from scipy import stats

data = np.array([...])
n = len(data)
mean = data.mean()
se = stats.sem(data)                        # 標準誤差

# 95% 信頼区間
ci = stats.t.interval(0.95, df=n-1, loc=mean, scale=se)
print(f"95% CI: ({ci[0]:.2f}, {ci[1]:.2f})")
```

---

## 6. 統計的検定

「差がある」「効果がある」を統計的に判断する方法。A/B テストの基盤。

### 仮説検定の流れ

| ステップ | 内容 |
|---------|------|
| 1. 帰無仮説 (H0) の設定 | 「差がない」「効果がない」という仮説 |
| 2. 対立仮説 (H1) の設定 | 「差がある」「効果がある」という仮説 |
| 3. 有意水準の設定 | 通常 alpha = 0.05（5%） |
| 4. 検定統計量の計算 | データから検定統計量を算出 |
| 5. p 値の算出 | H0 が正しいとき、観測データ以上に極端な結果が得られる確率 |
| 6. 判定 | p < alpha なら H0 を棄却（統計的に有意） |

### 主要な検定

| 検定 | 用途 | Python |
|------|------|--------|
| **t 検定（独立 2 群）** | 2 群の平均値の差 | `stats.ttest_ind(a, b)` |
| **対応のある t 検定** | 同一対象の前後比較 | `stats.ttest_rel(before, after)` |
| **1 標本 t 検定** | 1 群の平均が特定の値と異なるか | `stats.ttest_1samp(data, mu)` |
| **カイ二乗検定** | カテゴリ変数間の独立性 | `stats.chi2_contingency(table)` |
| **Mann-Whitney U 検定** | 2 群の比較（正規分布を仮定しない） | `stats.mannwhitneyu(a, b)` |
| **ANOVA（分散分析）** | 3 群以上の平均値の差 | `stats.f_oneway(a, b, c)` |

### A/B テストの実装

```python
from scipy import stats

# コントロール群とテスト群のコンバージョンデータ
control = np.array([0, 1, 0, 0, 1, ...])   # 0: 非コンバージョン, 1: コンバージョン
test = np.array([1, 1, 0, 1, 0, ...])

# 方法 1: t 検定
t_stat, p_value = stats.ttest_ind(control, test)
print(f"p-value: {p_value:.4f}")

# 方法 2: 比率の検定（Z 検定）
from statsmodels.stats.proportion import proportions_ztest

count = np.array([control.sum(), test.sum()])
nobs = np.array([len(control), len(test)])
z_stat, p_value = proportions_ztest(count, nobs)
print(f"p-value: {p_value:.4f}")

# 効果量（Cohen's d）
def cohens_d(a, b):
    pooled_std = np.sqrt((a.std()**2 + b.std()**2) / 2)
    return (a.mean() - b.mean()) / pooled_std

print(f"Effect size (Cohen's d): {cohens_d(control, test):.4f}")
```

**A/B テストの注意点:**

| 注意点 | 説明 |
|--------|------|
| サンプルサイズ | 十分なサンプル数がないと検出力が低い。事前にサンプルサイズ設計を行う |
| 多重比較 | 複数の指標を同時に検定すると偽陽性率が上がる。Bonferroni 補正等で調整 |
| p 値の誤解 | p < 0.05 は「効果がない確率が 5% 以下」ではない。「H0 が正しいとき、この結果以上が出る確率」 |
| 実用的有意性 | 統計的に有意でも、効果量が小さければ実用的に意味がない |

---

## 7. 回帰分析

DS の最も基本的なモデリング手法。ML の出発点。

### 線形回帰

```
y = w0 + w1*x1 + w2*x2 + ... + wn*xn + e

y: 目的変数
x1...xn: 説明変数
w0: 切片（バイアス）
w1...wn: 係数（重み）
e: 誤差項
```

```python
from sklearn.linear_model import LinearRegression

model = LinearRegression()
model.fit(X_train, y_train)

# 係数の確認
for name, coef in zip(X.columns, model.coef_):
    print(f"{name}: {coef:.4f}")
print(f"Intercept: {model.intercept_:.4f}")
print(f"R2 Score: {model.score(X_test, y_test):.4f}")
```

### ロジスティック回帰

```
P(y=1|x) = 1 / (1 + exp(-(w0 + w1*x1 + ... + wn*xn)))

シグモイド関数で確率 [0, 1] に変換
```

```python
from sklearn.linear_model import LogisticRegression

model = LogisticRegression(max_iter=1000)
model.fit(X_train, y_train)

# 確率の予測
probabilities = model.predict_proba(X_test)[:, 1]
```

### 正則化 — 過学習の防止

| 正則化 | ペナルティ | 効果 | sklearn のモデル |
|--------|----------|------|----------------|
| **L1（Lasso）** | 係数の絶対値の和 | 不要な特徴量の係数を 0 にする（特徴量選択） | `Lasso`, `LogisticRegression(penalty='l1')` |
| **L2（Ridge）** | 係数の二乗の和 | 係数を全体的に小さくする | `Ridge`, `LogisticRegression(penalty='l2')` |
| **Elastic Net** | L1 + L2 の組み合わせ | L1 と L2 のバランスを取る | `ElasticNet` |

```python
from sklearn.linear_model import Ridge, Lasso

# Ridge 回帰（L2 正則化）
ridge = Ridge(alpha=1.0)
ridge.fit(X_train, y_train)

# Lasso 回帰（L1 正則化）
lasso = Lasso(alpha=0.01)
lasso.fit(X_train, y_train)

# L1 で 0 になった特徴量を確認
zero_features = X.columns[lasso.coef_ == 0]
print(f"Removed features: {list(zero_features)}")
```

---

## 8. 推奨学習リソース

### 書籍

| 書籍 | 著者 | 対象 | 内容 |
|------|------|------|------|
| **統計学入門（東大出版会）** | 東京大学教養学部統計学教室 | 入門 | 統計学の定番教科書。日本語 |
| **データ分析のための統計学入門** | OpenIntro | 入門 | 無料で公開。実データを使った解説 |
| **Mathematics for Machine Learning** | Deisenroth 他 | 中級 | ML に必要な数学を網羅。無料 PDF あり |
| **パターン認識と機械学習（PRML）** | Bishop | 上級 | ML の理論的バイブル |
| **統計的学習の基礎（ESL）** | Hastie 他 | 上級 | 統計的学習の包括的教科書。無料 PDF あり |

### 動画

| リソース | URL | 内容 |
|---------|-----|------|
| **3Blue1Brown — Essence of Linear Algebra** | https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab | 線形代数の直感的な理解（ビジュアル） |
| **3Blue1Brown — Essence of Calculus** | https://www.youtube.com/playlist?list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr | 微積分の直感的な理解 |
| **StatQuest** | https://www.youtube.com/c/joshstarmer | 統計・ML の概念をわかりやすく解説 |
| **Khan Academy — Statistics** | https://www.khanacademy.org/math/statistics-probability | 統計学の基礎を体系的に学習 |
| **予備校のノリで学ぶ大学数学** | YouTube で検索 | 線形代数・統計学を日本語でわかりやすく |

### Web サイト

| サイト | URL | 内容 |
|--------|-----|------|
| **Seeing Theory** | https://seeing-theory.brown.edu | 確率・統計をインタラクティブに可視化 |
| **Setosa.io — Explained Visually** | https://setosa.io/ev/ | 統計・ML の概念をビジュアルで解説 |
| **Distill.pub** | https://distill.pub | ML の概念を美しいビジュアルで解説（論文形式） |

---

## 9. まとめ — 必要な数学レベル

### DS のレベル別必要度マップ

| 数学トピック | EDA・分析 | ML（scikit-learn） | DL（PyTorch） | 研究 |
|------------|----------|-------------------|-------------|------|
| 記述統計 | 必須 | 必須 | 必須 | 必須 |
| 確率分布 | 重要 | 必須 | 必須 | 必須 |
| 仮説検定 | 必須 | あると良い | あると良い | 必須 |
| 線形代数の基礎 | あると良い | 必須 | 必須 | 必須 |
| 微積分 | 不要 | あると良い | 必須 | 必須 |
| 最適化 | 不要 | あると良い | 必須 | 必須 |
| ベイズ統計 | あると良い | あると良い | あると良い | 必須 |
| 情報理論 | 不要 | あると良い | 重要 | 必須 |

### 最短ルート（実務で使うレベル）

| 順番 | トピック | 学習リソース | 目安期間 |
|------|---------|------------|---------|
| 1 | 記述統計・確率の基礎 | Khan Academy or 統計学入門 | 2 週間 |
| 2 | 仮説検定・信頼区間 | StatQuest + Python 実装 | 2 週間 |
| 3 | 線形代数の直感 | 3Blue1Brown の動画シリーズ | 1 週間 |
| 4 | 回帰分析 | scikit-learn で実装しながら学ぶ | 1 週間 |
| 5 | 勾配降下法の直感 | 3Blue1Brown Neural Networks + 実装 | 1 週間 |

> **原則**: 数学を完璧にしてから ML を始める必要はない。
> **「使いながら学ぶ」** のが最も効率的。scikit-learn でモデルを動かしながら、背景の数学を理解していく。

---

**関連ガイド:**

- [12 - DS 向け Web サイトガイド](12-web-resources-guide.md) — 学習プラットフォーム・論文サイト
- [15 - DS ワークフローガイド](15-workflow-guide.md) — EDA からデプロイまでの実践フロー
- [13 - DS 向け SNS・情報発信ガイド](13-sns-guide.md) — ML 学習チャンネル（YouTube）
